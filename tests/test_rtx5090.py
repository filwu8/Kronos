#!/usr/bin/env python3
"""
æµ‹è¯•RTX 5090 GPUæ”¯æŒ
"""

import torch
import time

def test_rtx5090():
    """æµ‹è¯•RTX 5090æ”¯æŒ"""
    print("ğŸš€ æµ‹è¯•RTX 5090 GPUæ”¯æŒ")
    print("=" * 50)
    
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"GPU {i}: {props.name}")
            print(f"  å†…å­˜: {props.total_memory / 1024**3:.1f} GB")
            print(f"  è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
            print(f"  å¤šå¤„ç†å™¨: {props.multi_processor_count}")
        
        # æµ‹è¯•GPUè®¡ç®—
        print("\nğŸ§ª GPUè®¡ç®—æµ‹è¯•:")
        try:
            device = torch.device("cuda:0")
            
            # ç®€å•æµ‹è¯•
            print("  æµ‹è¯•1: åŸºç¡€çŸ©é˜µè¿ç®—...")
            x = torch.randn(100, 100, device=device)
            y = torch.randn(100, 100, device=device)
            z = torch.mm(x, y)
            print("  âœ… åŸºç¡€è¿ç®—æˆåŠŸ")
            
            # æ€§èƒ½æµ‹è¯•
            print("  æµ‹è¯•2: æ€§èƒ½æµ‹è¯•...")
            size = 2000
            start_time = time.time()
            
            x = torch.randn(size, size, device=device)
            y = torch.randn(size, size, device=device)
            z = torch.mm(x, y)
            torch.cuda.synchronize()
            
            end_time = time.time()
            gpu_time = end_time - start_time
            
            print(f"  âœ… {size}x{size} çŸ©é˜µä¹˜æ³•: {gpu_time:.3f}s")
            
            # å†…å­˜æµ‹è¯•
            print("  æµ‹è¯•3: å†…å­˜ä½¿ç”¨...")
            memory_allocated = torch.cuda.memory_allocated(device) / 1024**3
            memory_reserved = torch.cuda.memory_reserved(device) / 1024**3
            memory_total = torch.cuda.get_device_properties(device).total_memory / 1024**3
            
            print(f"  å·²åˆ†é…: {memory_allocated:.2f} GB")
            print(f"  å·²ä¿ç•™: {memory_reserved:.2f} GB")
            print(f"  æ€»å†…å­˜: {memory_total:.2f} GB")
            print(f"  ä½¿ç”¨ç‡: {(memory_allocated/memory_total)*100:.1f}%")
            
            # æ¸…ç†
            del x, y, z
            torch.cuda.empty_cache()
            
            print("\nğŸ‰ RTX 5090 GPUæµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
            return True
            
        except Exception as e:
            print(f"  âŒ GPUæµ‹è¯•å¤±è´¥: {str(e)}")
            return False
    else:
        print("âŒ CUDAä¸å¯ç”¨")
        return False

if __name__ == "__main__":
    success = test_rtx5090()
    
    if success:
        print("\nâœ… RTX 5090å·²å°±ç»ªï¼")
        print("ç°åœ¨å¯ä»¥é‡å¯APIæœåŠ¡ä½¿ç”¨GPUåŠ é€Ÿ")
    else:
        print("\nâŒ GPUæµ‹è¯•å¤±è´¥")
        print("å°†ç»§ç»­ä½¿ç”¨CPUæ¨¡å¼")
