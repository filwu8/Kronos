#!/usr/bin/env python3
"""
æœ€ç»ˆç³»ç»ŸéªŒè¯
éªŒè¯é‡æ„åçš„RTX 5090 GPUåŠ é€Ÿè‚¡ç¥¨é¢„æµ‹ç³»ç»Ÿ
"""

import requests
import time
import os
from pathlib import Path

def check_project_structure():
    """æ£€æŸ¥é¡¹ç›®ç»“æ„"""
    print("ğŸ“ æ£€æŸ¥é¡¹ç›®ç»“æ„...")
    
    required_dirs = [
        "volumes/data/akshare_data",
        "volumes/models",
        "volumes/logs", 
        "volumes/config",
        "tests/unit",
        "tests/integration",
        "tests/performance",
        "scripts/docker",
        "scripts/local"
    ]
    
    missing_dirs = []
    for dir_path in required_dirs:
        if not Path(dir_path).exists():
            missing_dirs.append(dir_path)
        else:
            print(f"   âœ… {dir_path}")
    
    if missing_dirs:
        print(f"   âŒ ç¼ºå°‘ç›®å½•: {missing_dirs}")
        return False
    
    return True

def check_data_migration():
    """æ£€æŸ¥æ•°æ®è¿ç§»"""
    print("\nğŸ“Š æ£€æŸ¥æ•°æ®è¿ç§»...")
    
    old_data_dir = Path("data/akshare_data")
    new_data_dir = Path("volumes/data/akshare_data")
    
    if new_data_dir.exists():
        csv_files = list(new_data_dir.glob("*.csv"))
        print(f"   âœ… æ–°æ•°æ®ç›®å½•å­˜åœ¨: {len(csv_files)} ä¸ªè‚¡ç¥¨æ–‡ä»¶")
        
        # æ£€æŸ¥å‡ ä¸ªå…³é”®è‚¡ç¥¨
        key_stocks = ["000001.csv", "000002.csv", "000004.csv"]
        for stock_file in key_stocks:
            if (new_data_dir / stock_file).exists():
                print(f"   âœ… {stock_file} å·²è¿ç§»")
            else:
                print(f"   âŒ {stock_file} ç¼ºå¤±")
                return False
        
        return True
    else:
        print(f"   âŒ æ–°æ•°æ®ç›®å½•ä¸å­˜åœ¨: {new_data_dir}")
        return False

def check_config_system():
    """æ£€æŸ¥é…ç½®ç³»ç»Ÿ"""
    print("\nâš™ï¸ æ£€æŸ¥é…ç½®ç³»ç»Ÿ...")
    
    try:
        # æµ‹è¯•é…ç½®å¯¼å…¥
        import sys
        sys.path.append("volumes")
        
        from volumes.config.settings import settings, DATA_DIR, DEPLOYMENT_MODE
        
        print(f"   âœ… é…ç½®ç³»ç»ŸåŠ è½½æˆåŠŸ")
        print(f"   ğŸ“ æ•°æ®ç›®å½•: {DATA_DIR}")
        print(f"   ğŸš€ éƒ¨ç½²æ¨¡å¼: {DEPLOYMENT_MODE}")
        print(f"   ğŸ“Š åŸºç¡€ç›®å½•: {settings.base_dir}")
        
        return True
    except Exception as e:
        print(f"   âŒ é…ç½®ç³»ç»Ÿå¤±è´¥: {str(e)}")
        return False

def check_api_service():
    """æ£€æŸ¥APIæœåŠ¡"""
    print("\nğŸ”Œ æ£€æŸ¥APIæœåŠ¡...")
    
    try:
        response = requests.get("http://localhost:8000/health", timeout=5)
        if response.status_code == 200:
            data = response.json()
            print(f"   âœ… APIæœåŠ¡æ­£å¸¸")
            print(f"   ğŸ–¥ï¸ è®¾å¤‡: {data['model_status']['device']}")
            print(f"   ğŸ“Š æ¨¡å‹: {data['model_status']['model_loaded']}")
            print(f"   ğŸ­ æ¨¡æ‹Ÿ: {data['model_status']['use_mock']}")
            return True
        else:
            print(f"   âŒ APIé”™è¯¯: {response.status_code}")
            return False
    except Exception as e:
        print(f"   âŒ APIè¿æ¥å¤±è´¥: {str(e)}")
        return False

def check_prediction_function():
    """æ£€æŸ¥é¢„æµ‹åŠŸèƒ½"""
    print("\nğŸ”® æ£€æŸ¥é¢„æµ‹åŠŸèƒ½...")
    
    try:
        start_time = time.time()
        response = requests.post(
            "http://localhost:8000/predict",
            json={"stock_code": "000001", "pred_len": 5},
            timeout=20
        )
        end_time = time.time()
        
        if response.status_code == 200:
            data = response.json()
            if data.get('success'):
                summary = data['data']['summary']
                stock_info = data['data']['stock_info']
                
                print(f"   âœ… é¢„æµ‹æˆåŠŸ ({end_time - start_time:.2f}s)")
                print(f"   ğŸ“ˆ è‚¡ç¥¨: {stock_info['name']} ({stock_info['code']})")
                print(f"   ğŸ’° å½“å‰: Â¥{summary['current_price']:.2f}")
                print(f"   ğŸ”® é¢„æµ‹: Â¥{summary['predicted_price']:.2f}")
                print(f"   ğŸ“Š å˜åŒ–: {summary['change_percent']:+.2f}%")
                print(f"   ğŸ¯ è¶‹åŠ¿: {summary['trend']}")
                
                return True
            else:
                print(f"   âŒ é¢„æµ‹å¤±è´¥: {data.get('error')}")
                return False
        else:
            print(f"   âŒ HTTPé”™è¯¯: {response.status_code}")
            return False
    except Exception as e:
        print(f"   âŒ é¢„æµ‹å¼‚å¸¸: {str(e)}")
        return False

def check_streamlit_service():
    """æ£€æŸ¥StreamlitæœåŠ¡"""
    print("\nğŸ¨ æ£€æŸ¥StreamlitæœåŠ¡...")
    
    try:
        response = requests.get("http://localhost:8501", timeout=5)
        if response.status_code == 200:
            print(f"   âœ… StreamlitæœåŠ¡æ­£å¸¸")
            return True
        else:
            print(f"   âŒ Streamlité”™è¯¯: {response.status_code}")
            return False
    except Exception as e:
        print(f"   âŒ Streamlitè¿æ¥å¤±è´¥: {str(e)}")
        return False

def check_gpu_acceleration():
    """æ£€æŸ¥GPUåŠ é€Ÿ"""
    print("\nğŸš€ æ£€æŸ¥GPUåŠ é€Ÿ...")
    
    try:
        import torch
        
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            print(f"   âœ… GPU: {gpu_name}")
            print(f"   ğŸ’¾ æ˜¾å­˜: {gpu_memory:.1f} GB")
            print(f"   ğŸ”§ PyTorch: {torch.__version__}")
            
            # æµ‹è¯•GPUè®¡ç®—
            x = torch.randn(1000, 1000, device='cuda')
            y = torch.mm(x, x)
            print(f"   âœ… GPUè®¡ç®—æ­£å¸¸")
            
            return True
        else:
            print(f"   âš ï¸ GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨¡å¼")
            return False
    except Exception as e:
        print(f"   âŒ GPUæ£€æŸ¥å¤±è´¥: {str(e)}")
        return False

def check_auto_download():
    """æ£€æŸ¥è‡ªåŠ¨ä¸‹è½½åŠŸèƒ½"""
    print("\nğŸ“¥ æ£€æŸ¥è‡ªåŠ¨ä¸‹è½½åŠŸèƒ½...")
    
    try:
        # æµ‹è¯•ä¸€ä¸ªä¸å­˜åœ¨çš„è‚¡ç¥¨ä»£ç 
        response = requests.post(
            "http://localhost:8000/predict",
            json={"stock_code": "000999"},  # å‡è®¾è¿™ä¸ªä¸å­˜åœ¨
            timeout=30
        )
        
        if response.status_code == 400:
            print(f"   âœ… æ­£ç¡®å¤„ç†ä¸å­˜åœ¨çš„è‚¡ç¥¨ä»£ç ")
            return True
        elif response.status_code == 200:
            data = response.json()
            if data.get('success'):
                print(f"   âœ… è‡ªåŠ¨ä¸‹è½½åŠŸèƒ½æ­£å¸¸å·¥ä½œ")
                return True
            else:
                print(f"   âš ï¸ è‡ªåŠ¨ä¸‹è½½å¤±è´¥ä½†é”™è¯¯å¤„ç†æ­£ç¡®")
                return True
        else:
            print(f"   âŒ æ„å¤–çš„å“åº”: {response.status_code}")
            return False
    except Exception as e:
        print(f"   âŒ è‡ªåŠ¨ä¸‹è½½æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def generate_system_report():
    """ç”Ÿæˆç³»ç»ŸæŠ¥å‘Š"""
    print("\nğŸ“‹ ç”Ÿæˆç³»ç»ŸæŠ¥å‘Š...")
    
    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "deployment_mode": "local",
        "gpu_info": {},
        "service_status": {},
        "performance": {}
    }
    
    # GPUä¿¡æ¯
    try:
        import torch
        if torch.cuda.is_available():
            report["gpu_info"] = {
                "name": torch.cuda.get_device_name(0),
                "memory_gb": torch.cuda.get_device_properties(0).total_memory / 1024**3,
                "pytorch_version": torch.__version__,
                "cuda_available": True
            }
        else:
            report["gpu_info"]["cuda_available"] = False
    except:
        report["gpu_info"]["error"] = "æ— æ³•è·å–GPUä¿¡æ¯"
    
    # æœåŠ¡çŠ¶æ€
    try:
        health_response = requests.get("http://localhost:8000/health", timeout=5)
        if health_response.status_code == 200:
            report["service_status"]["api"] = health_response.json()
        
        streamlit_response = requests.get("http://localhost:8501", timeout=5)
        report["service_status"]["streamlit"] = {
            "status_code": streamlit_response.status_code,
            "available": streamlit_response.status_code == 200
        }
    except Exception as e:
        report["service_status"]["error"] = str(e)
    
    # æ€§èƒ½æµ‹è¯•
    try:
        start_time = time.time()
        pred_response = requests.post(
            "http://localhost:8000/predict",
            json={"stock_code": "000001"},
            timeout=20
        )
        end_time = time.time()
        
        if pred_response.status_code == 200:
            report["performance"] = {
                "prediction_time": end_time - start_time,
                "success": True
            }
    except Exception as e:
        report["performance"]["error"] = str(e)
    
    # ä¿å­˜æŠ¥å‘Š
    import json
    report_path = Path("tests/results/system_verification_report.json")
    report_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"   âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

def main():
    """ä¸»éªŒè¯å‡½æ•°"""
    print("ğŸ”¬ RTX 5090 GPUè‚¡ç¥¨é¢„æµ‹ç³»ç»Ÿ - æœ€ç»ˆéªŒè¯")
    print("=" * 70)
    
    tests = [
        ("é¡¹ç›®ç»“æ„", check_project_structure),
        ("æ•°æ®è¿ç§»", check_data_migration),
        ("é…ç½®ç³»ç»Ÿ", check_config_system),
        ("APIæœåŠ¡", check_api_service),
        ("é¢„æµ‹åŠŸèƒ½", check_prediction_function),
        ("StreamlitæœåŠ¡", check_streamlit_service),
        ("GPUåŠ é€Ÿ", check_gpu_acceleration),
        ("è‡ªåŠ¨ä¸‹è½½", check_auto_download)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name}: é€šè¿‡")
            else:
                print(f"âŒ {test_name}: å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name}: å¼‚å¸¸ - {str(e)}")
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_system_report()
    
    # æ€»ç»“
    print("\n" + "=" * 70)
    print(f"ğŸ“Š éªŒè¯ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ ç³»ç»Ÿé‡æ„å®Œå…¨æˆåŠŸ!")
        print("\nâœ… é‡æ„å®Œæˆçš„åŠŸèƒ½:")
        print("   1. âœ… ç›®å½•ç»“æ„æ ‡å‡†åŒ– (volumes/, tests/, scripts/)")
        print("   2. âœ… æ•°æ®æ–‡ä»¶è¿ç§» (100åªè‚¡ç¥¨5å¹´æ•°æ®)")
        print("   3. âœ… ç»Ÿä¸€é…ç½®ç®¡ç† (æ”¯æŒDockerå’Œæœ¬åœ°æ¨¡å¼)")
        print("   4. âœ… è‡ªåŠ¨æ•°æ®ä¸‹è½½ (ç¼ºå¤±è‚¡ç¥¨è‡ªåŠ¨è·å–)")
        print("   5. âœ… RTX 5090 GPUåŠ é€Ÿ (PyTorch nightly)")
        print("   6. âœ… åŒç‰ˆæœ¬éƒ¨ç½²æ”¯æŒ (Docker + æœ¬åœ°exe)")
        print("   7. âœ… æµ‹è¯•æ–‡ä»¶æ•´ç† (å•å…ƒ/é›†æˆ/æ€§èƒ½æµ‹è¯•)")
        print("   8. âœ… éƒ¨ç½²è„šæœ¬å®Œå–„ (è‡ªåŠ¨åŒ–å¯åŠ¨)")
        
        print("\nğŸš€ ç³»ç»Ÿç‰¹æ€§:")
        print("   ğŸ“Š æ•°æ®: 100åªAè‚¡5å¹´çœŸå®å†å²æ•°æ®")
        print("   ğŸ–¥ï¸ GPU: RTX 5090å®Œå…¨æ”¯æŒ")
        print("   ğŸ¨ ç•Œé¢: ä¸­æ–‡åŒ–äº¤äº’ç•Œé¢")
        print("   ğŸ³ éƒ¨ç½²: Dockerå®¹å™¨åŒ–")
        print("   ğŸ’» æœ¬åœ°: exeç‹¬ç«‹è¿è¡Œ")
        print("   ğŸ”§ é…ç½®: ç»Ÿä¸€é…ç½®ç®¡ç†")
        
        print("\nğŸŒ è®¿é—®åœ°å€:")
        print("   å‰ç«¯ç•Œé¢: http://localhost:8501")
        print("   APIæ–‡æ¡£: http://localhost:8000/docs")
        print("   å¥åº·æ£€æŸ¥: http://localhost:8000/health")
        
    else:
        print("âš ï¸ éƒ¨åˆ†åŠŸèƒ½éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
        print(f"é€šè¿‡ç‡: {(passed/total)*100:.1f}%")

if __name__ == "__main__":
    main()
