#!/usr/bin/env python3
"""
ç›´æ¥æµ‹è¯•GPUåŠ é€Ÿç³»ç»Ÿ
"""

import requests
import time
import torch

def test_gpu_status():
    """æµ‹è¯•GPUçŠ¶æ€"""
    print("ğŸ” GPUçŠ¶æ€æ£€æŸ¥:")
    print(f"   PyTorch: {torch.__version__}")
    print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        return True
    return False

def test_api_health():
    """æµ‹è¯•APIå¥åº·"""
    print("\nğŸ¥ APIå¥åº·æ£€æŸ¥:")
    
    try:
        response = requests.get('http://localhost:8000/health', timeout=5)
        if response.status_code == 200:
            data = response.json()
            print(f"   âœ… APIæ­£å¸¸è¿è¡Œ")
            print(f"   è®¾å¤‡: {data['model_status']['device']}")
            print(f"   æ¨¡å‹: {data['model_status']['model_loaded']}")
            print(f"   æ¨¡æ‹Ÿ: {data['model_status']['use_mock']}")
            return True
        else:
            print(f"   âŒ APIé”™è¯¯: {response.status_code}")
            return False
    except Exception as e:
        print(f"   âŒ è¿æ¥å¤±è´¥: {str(e)}")
        return False

def test_single_prediction():
    """æµ‹è¯•å•æ¬¡é¢„æµ‹"""
    print("\nğŸ“Š å•æ¬¡é¢„æµ‹æµ‹è¯•:")
    
    try:
        print("   å‘é€é¢„æµ‹è¯·æ±‚...")
        start_time = time.time()
        
        response = requests.post(
            'http://localhost:8000/predict',
            json={'stock_code': '000001'},
            timeout=30
        )
        
        end_time = time.time()
        duration = end_time - start_time
        
        if response.status_code == 200:
            data = response.json()
            
            if data.get('success'):
                stock_info = data['data']['stock_info']
                summary = data['data']['summary']
                
                print(f"   âœ… é¢„æµ‹æˆåŠŸ ({duration:.2f}ç§’)")
                print(f"   ğŸ“ˆ {stock_info['name']} ({stock_info['code']})")
                print(f"   ğŸ’° å½“å‰: Â¥{summary['current_price']:.2f}")
                print(f"   ğŸ”® é¢„æµ‹: Â¥{summary['predicted_price']:.2f}")
                print(f"   ğŸ“Š å˜åŒ–: {summary['change_percent']:+.2f}%")
                print(f"   ğŸ¯ è¶‹åŠ¿: {summary['trend']}")
                
                return duration
            else:
                print(f"   âŒ é¢„æµ‹å¤±è´¥: {data.get('error')}")
                return None
        else:
            print(f"   âŒ HTTPé”™è¯¯: {response.status_code}")
            return None
            
    except Exception as e:
        print(f"   âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")
        return None

def test_gpu_memory():
    """æµ‹è¯•GPUå†…å­˜ä½¿ç”¨"""
    print("\nğŸ’¾ GPUå†…å­˜ç›‘æ§:")
    
    if not torch.cuda.is_available():
        print("   âŒ GPUä¸å¯ç”¨")
        return
    
    try:
        device = torch.device("cuda:0")
        
        # è·å–å†…å­˜ä¿¡æ¯
        allocated = torch.cuda.memory_allocated(device) / 1024**3
        reserved = torch.cuda.memory_reserved(device) / 1024**3
        total = torch.cuda.get_device_properties(device).total_memory / 1024**3
        
        print(f"   å·²åˆ†é…: {allocated:.2f} GB")
        print(f"   å·²ä¿ç•™: {reserved:.2f} GB")
        print(f"   æ€»å†…å­˜: {total:.2f} GB")
        print(f"   ä½¿ç”¨ç‡: {(allocated/total)*100:.1f}%")
        
    except Exception as e:
        print(f"   âŒ ç›‘æ§å¤±è´¥: {str(e)}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ RTX 5090 GPUåŠ é€Ÿç³»ç»Ÿç›´æ¥æµ‹è¯•")
    print("=" * 60)
    
    # 1. GPUçŠ¶æ€
    gpu_ok = test_gpu_status()
    
    # 2. APIå¥åº·
    api_ok = test_api_health()
    
    # 3. é¢„æµ‹æµ‹è¯•
    prediction_time = None
    if api_ok:
        prediction_time = test_single_prediction()
    
    # 4. GPUå†…å­˜
    test_gpu_memory()
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“‹ æµ‹è¯•ç»“æœæ€»ç»“:")
    
    if gpu_ok:
        print("âœ… RTX 5090 GPU: å®Œå…¨å¯ç”¨")
    else:
        print("âŒ GPU: ä¸å¯ç”¨")
    
    if api_ok:
        print("âœ… APIæœåŠ¡: æ­£å¸¸è¿è¡Œ")
    else:
        print("âŒ APIæœåŠ¡: å¼‚å¸¸")
    
    if prediction_time:
        print(f"âœ… é¢„æµ‹åŠŸèƒ½: æ­£å¸¸ ({prediction_time:.2f}ç§’)")
        
        if prediction_time < 3.0:
            print("ğŸš€ æ€§èƒ½: ä¼˜ç§€ (GPUåŠ é€Ÿæ•ˆæœæ˜æ˜¾)")
        elif prediction_time < 5.0:
            print("âš¡ æ€§èƒ½: è‰¯å¥½")
        else:
            print("ğŸŒ æ€§èƒ½: ä¸€èˆ¬ (å¯èƒ½æœªä½¿ç”¨GPU)")
    else:
        print("âŒ é¢„æµ‹åŠŸèƒ½: å¤±è´¥")
    
    # æœ€ç»ˆçŠ¶æ€
    if gpu_ok and api_ok and prediction_time:
        print(f"\nğŸ‰ ç³»ç»ŸçŠ¶æ€: å®Œç¾è¿è¡Œ")
        print(f"   ğŸš€ RTX 5090å·²å®Œå…¨å¯ç”¨")
        print(f"   âš¡ GPUåŠ é€Ÿæ­£å¸¸å·¥ä½œ")
        print(f"   ğŸ“Š é¢„æµ‹åŠŸèƒ½å®Œå…¨æ­£å¸¸")
        print(f"   ğŸŒ å¯è®¿é—®: http://localhost:8501")
    else:
        print(f"\nâš ï¸ ç³»ç»ŸçŠ¶æ€: éœ€è¦æ£€æŸ¥")
        if not gpu_ok:
            print(f"   ğŸ”§ GPUé—®é¢˜éœ€è¦è§£å†³")
        if not api_ok:
            print(f"   ğŸ”§ APIæœåŠ¡éœ€è¦é‡å¯")
        if not prediction_time:
            print(f"   ğŸ”§ é¢„æµ‹åŠŸèƒ½éœ€è¦è°ƒè¯•")

if __name__ == "__main__":
    main()
