#!/usr/bin/env python3
"""
éªŒè¯GPUéƒ¨ç½²çŠ¶æ€
"""

import requests
import torch
import time

def check_gpu_status():
    """æ£€æŸ¥GPUçŠ¶æ€"""
    print("ğŸ” GPUç¡¬ä»¶çŠ¶æ€:")
    print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"   è®¡ç®—èƒ½åŠ›: {torch.cuda.get_device_properties(0).major}.{torch.cuda.get_device_properties(0).minor}")
        return True
    return False

def check_api_gpu_usage():
    """æ£€æŸ¥API GPUä½¿ç”¨"""
    print("\nğŸ” API GPUä½¿ç”¨çŠ¶æ€:")
    
    try:
        response = requests.get('http://localhost:8000/health', timeout=5)
        if response.status_code == 200:
            data = response.json()
            device = data['model_status']['device']
            model_loaded = data['model_status']['model_loaded']
            use_mock = data['model_status']['use_mock']
            
            print(f"   è®¾å¤‡: {device}")
            print(f"   æ¨¡å‹åŠ è½½: {model_loaded}")
            print(f"   ä½¿ç”¨æ¨¡æ‹Ÿ: {use_mock}")
            
            if device == "cuda":
                print("   âœ… APIæ­£åœ¨ä½¿ç”¨GPU")
                return True
            else:
                print("   ğŸ–¥ï¸ APIæ­£åœ¨ä½¿ç”¨CPU")
                return False
        else:
            print(f"   âŒ APIå¥åº·æ£€æŸ¥å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        print(f"   âŒ APIè¿æ¥å¤±è´¥: {str(e)}")
        return False

def test_prediction_performance():
    """æµ‹è¯•é¢„æµ‹æ€§èƒ½"""
    print("\nâš¡ é¢„æµ‹æ€§èƒ½æµ‹è¯•:")
    
    try:
        start_time = time.time()
        response = requests.post(
            'http://localhost:8000/predict',
            json={'stock_code': '000001', 'pred_len': 10},
            timeout=60
        )
        end_time = time.time()
        
        if response.status_code == 200:
            data = response.json()
            if data.get('success'):
                prediction_time = end_time - start_time
                print(f"   âœ… é¢„æµ‹æˆåŠŸ")
                print(f"   â±ï¸ è€—æ—¶: {prediction_time:.2f}ç§’")
                
                stock_info = data['data']['stock_info']
                summary = data['data']['summary']
                print(f"   ğŸ“Š è‚¡ç¥¨: {stock_info['name']}")
                print(f"   ğŸ’° å½“å‰: Â¥{summary['current_price']:.2f}")
                print(f"   ğŸ“ˆ é¢„æµ‹: Â¥{summary['predicted_price']:.2f}")
                
                return prediction_time
            else:
                print(f"   âŒ é¢„æµ‹å¤±è´¥: {data.get('error')}")
                return None
        else:
            print(f"   âŒ HTTPé”™è¯¯: {response.status_code}")
            return None
    except Exception as e:
        print(f"   âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        return None

def monitor_gpu_usage():
    """ç›‘æ§GPUä½¿ç”¨"""
    print("\nğŸ“Š GPUä½¿ç”¨ç›‘æ§:")
    
    if not torch.cuda.is_available():
        print("   âŒ GPUä¸å¯ç”¨")
        return
    
    try:
        device = torch.device("cuda:0")
        
        # å†…å­˜ä½¿ç”¨
        memory_allocated = torch.cuda.memory_allocated(device) / 1024**3
        memory_reserved = torch.cuda.memory_reserved(device) / 1024**3
        memory_total = torch.cuda.get_device_properties(device).total_memory / 1024**3
        
        print(f"   ğŸ’¾ å·²åˆ†é…: {memory_allocated:.2f} GB")
        print(f"   ğŸ’¾ å·²ä¿ç•™: {memory_reserved:.2f} GB")
        print(f"   ğŸ’¾ æ€»å†…å­˜: {memory_total:.2f} GB")
        print(f"   ğŸ“Š ä½¿ç”¨ç‡: {(memory_allocated/memory_total)*100:.1f}%")
        
    except Exception as e:
        print(f"   âŒ ç›‘æ§å¤±è´¥: {str(e)}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ GPUéƒ¨ç½²éªŒè¯")
    print("=" * 60)
    
    # 1. æ£€æŸ¥GPUç¡¬ä»¶
    gpu_available = check_gpu_status()
    
    # 2. æ£€æŸ¥API GPUä½¿ç”¨
    api_using_gpu = check_api_gpu_usage()
    
    # 3. æµ‹è¯•é¢„æµ‹æ€§èƒ½
    prediction_time = test_prediction_performance()
    
    # 4. ç›‘æ§GPUä½¿ç”¨
    monitor_gpu_usage()
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š GPUéƒ¨ç½²çŠ¶æ€æ€»ç»“:")
    
    if gpu_available:
        print("âœ… GPUç¡¬ä»¶: RTX 5090 å¯ç”¨")
        print(f"âœ… PyTorch: {torch.__version__} (æ”¯æŒRTX 5090)")
    else:
        print("âŒ GPUç¡¬ä»¶: ä¸å¯ç”¨")
    
    if api_using_gpu:
        print("âœ… APIæœåŠ¡: ä½¿ç”¨GPUåŠ é€Ÿ")
    else:
        print("ğŸ–¥ï¸ APIæœåŠ¡: ä½¿ç”¨CPUæ¨¡å¼")
    
    if prediction_time:
        print(f"âœ… é¢„æµ‹æ€§èƒ½: {prediction_time:.2f}ç§’")
        
        if api_using_gpu:
            print("ğŸš€ GPUåŠ é€Ÿæ•ˆæœ: é¢„æµ‹é€Ÿåº¦æ˜¾è‘—æå‡")
        else:
            print("ğŸ’¡ å»ºè®®: å¯ç”¨GPUå¯è·å¾—æ›´å¥½æ€§èƒ½")
    else:
        print("âŒ é¢„æµ‹æµ‹è¯•: å¤±è´¥")
    
    # æœ€ç»ˆå»ºè®®
    print(f"\nğŸ’¡ éƒ¨ç½²å»ºè®®:")
    
    if gpu_available and api_using_gpu:
        print("ğŸ‰ å®Œç¾ï¼æ‚¨çš„RTX 5090å·²å®Œå…¨å¯ç”¨")
        print("   - äº«å—é¡¶çº§GPUæ€§èƒ½")
        print("   - é¢„æµ‹é€Ÿåº¦æå¿«")
        print("   - æ”¯æŒå¤§è§„æ¨¡æ‰¹é‡é¢„æµ‹")
    elif gpu_available and not api_using_gpu:
        print("âš ï¸ GPUå¯ç”¨ä½†APIæœªä½¿ç”¨")
        print("   - æ£€æŸ¥APIé…ç½®")
        print("   - é‡å¯APIæœåŠ¡")
        print("   - ç¡®è®¤CUDAç¯å¢ƒ")
    else:
        print("ğŸ–¥ï¸ å½“å‰ä½¿ç”¨CPUæ¨¡å¼")
        print("   - åŠŸèƒ½å®Œå…¨æ­£å¸¸")
        print("   - æ€§èƒ½å·²ä¼˜åŒ–")
        print("   - ç­‰å¾…GPUæ”¯æŒæ›´æ–°")

if __name__ == "__main__":
    main()
