#!/usr/bin/env python3
"""
GPUæ€§èƒ½æµ‹è¯•
"""

import torch
import time
import numpy as np
import requests
from datetime import datetime

def test_gpu_availability():
    """æµ‹è¯•GPUå¯ç”¨æ€§"""
    print("ğŸ” GPUå¯ç”¨æ€§æ£€æµ‹")
    print("=" * 50)
    
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"GPU {i}: {props.name}")
            print(f"  å†…å­˜: {props.total_memory / 1024**3:.1f} GB")
            print(f"  è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
            print(f"  å¤šå¤„ç†å™¨: {props.multi_processor_count}")
        
        # æµ‹è¯•GPUå†…å­˜
        try:
            device = torch.device("cuda:0")
            x = torch.randn(1000, 1000, device=device)
            y = torch.randn(1000, 1000, device=device)
            z = torch.mm(x, y)
            print(f"âœ… GPUè®¡ç®—æµ‹è¯•é€šè¿‡")
            
            # æ¸…ç†å†…å­˜
            del x, y, z
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"âŒ GPUè®¡ç®—æµ‹è¯•å¤±è´¥: {str(e)}")
            return False
        
        return True
    else:
        print("âŒ CUDAä¸å¯ç”¨")
        return False

def benchmark_cpu_vs_gpu():
    """CPU vs GPUæ€§èƒ½å¯¹æ¯”"""
    print("\nâš¡ CPU vs GPUæ€§èƒ½å¯¹æ¯”")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("âŒ GPUä¸å¯ç”¨ï¼Œè·³è¿‡æ€§èƒ½å¯¹æ¯”")
        return
    
    # æµ‹è¯•çŸ©é˜µè¿ç®—æ€§èƒ½
    size = 2000
    iterations = 10
    
    print(f"æµ‹è¯•: {size}x{size} çŸ©é˜µä¹˜æ³•ï¼Œ{iterations} æ¬¡è¿­ä»£")
    
    # CPUæµ‹è¯•
    print("\nğŸ–¥ï¸ CPUæ€§èƒ½æµ‹è¯•...")
    cpu_times = []
    
    for i in range(iterations):
        start_time = time.time()
        
        x_cpu = torch.randn(size, size)
        y_cpu = torch.randn(size, size)
        z_cpu = torch.mm(x_cpu, y_cpu)
        
        end_time = time.time()
        cpu_times.append(end_time - start_time)
        
        if i == 0:
            print(f"  ç¬¬1æ¬¡: {cpu_times[0]:.3f}s")
    
    cpu_avg = np.mean(cpu_times)
    print(f"  å¹³å‡æ—¶é—´: {cpu_avg:.3f}s")
    
    # GPUæµ‹è¯•
    print("\nğŸš€ GPUæ€§èƒ½æµ‹è¯•...")
    gpu_times = []
    device = torch.device("cuda:0")
    
    # é¢„çƒ­GPU
    x_gpu = torch.randn(size, size, device=device)
    y_gpu = torch.randn(size, size, device=device)
    torch.mm(x_gpu, y_gpu)
    torch.cuda.synchronize()
    
    for i in range(iterations):
        torch.cuda.synchronize()
        start_time = time.time()
        
        x_gpu = torch.randn(size, size, device=device)
        y_gpu = torch.randn(size, size, device=device)
        z_gpu = torch.mm(x_gpu, y_gpu)
        
        torch.cuda.synchronize()
        end_time = time.time()
        gpu_times.append(end_time - start_time)
        
        if i == 0:
            print(f"  ç¬¬1æ¬¡: {gpu_times[0]:.3f}s")
    
    gpu_avg = np.mean(gpu_times)
    print(f"  å¹³å‡æ—¶é—´: {gpu_avg:.3f}s")
    
    # æ€§èƒ½å¯¹æ¯”
    speedup = cpu_avg / gpu_avg
    print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
    print(f"  CPUå¹³å‡: {cpu_avg:.3f}s")
    print(f"  GPUå¹³å‡: {gpu_avg:.3f}s")
    print(f"  åŠ é€Ÿæ¯”: {speedup:.1f}x")
    
    if speedup > 1:
        print(f"  ğŸš€ GPUæ¯”CPUå¿« {speedup:.1f} å€")
    else:
        print(f"  âš ï¸ GPUæ€§èƒ½æœªè¾¾é¢„æœŸ")
    
    # æ¸…ç†GPUå†…å­˜
    torch.cuda.empty_cache()

def test_api_with_gpu():
    """æµ‹è¯•APIä½¿ç”¨GPU"""
    print("\nğŸ”Œ æµ‹è¯•API GPUä½¿ç”¨")
    print("=" * 50)
    
    # é‡å¯APIæœåŠ¡ä»¥ä½¿ç”¨GPUé…ç½®
    print("è¯·é‡å¯APIæœåŠ¡ä»¥åº”ç”¨GPUé…ç½®...")
    print("å‘½ä»¤: python -m uvicorn app.api:app --host 0.0.0.0 --port 8000")
    
    # ç­‰å¾…ç”¨æˆ·é‡å¯æœåŠ¡
    input("æŒ‰å›è½¦é”®ç»§ç»­æµ‹è¯•API...")
    
    try:
        # æµ‹è¯•å¥åº·æ£€æŸ¥
        response = requests.get("http://localhost:8000/health", timeout=5)
        if response.status_code == 200:
            data = response.json()
            device = data['model_status']['device']
            print(f"âœ… APIæœåŠ¡è¿è¡Œæ­£å¸¸")
            print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
            
            if device == "cuda":
                print(f"ğŸš€ APIæ­£åœ¨ä½¿ç”¨GPU")
            else:
                print(f"ğŸ–¥ï¸ APIæ­£åœ¨ä½¿ç”¨CPU")
        else:
            print(f"âŒ APIå¥åº·æ£€æŸ¥å¤±è´¥: {response.status_code}")
            return False
        
        # æµ‹è¯•é¢„æµ‹æ€§èƒ½
        print(f"\nâ±ï¸ æµ‹è¯•é¢„æµ‹æ€§èƒ½...")
        
        start_time = time.time()
        response = requests.post(
            "http://localhost:8000/predict",
            json={"stock_code": "000001", "pred_len": 10},
            timeout=60
        )
        end_time = time.time()
        
        if response.status_code == 200:
            data = response.json()
            if data.get('success'):
                prediction_time = end_time - start_time
                print(f"âœ… é¢„æµ‹æˆåŠŸ")
                print(f"â±ï¸ é¢„æµ‹è€—æ—¶: {prediction_time:.2f}s")
                
                stock_info = data['data']['stock_info']
                summary = data['data']['summary']
                print(f"ğŸ“Š è‚¡ç¥¨: {stock_info['name']}")
                print(f"ğŸ’° å½“å‰ä»·æ ¼: Â¥{summary['current_price']:.2f}")
                print(f"ğŸ“ˆ é¢„æµ‹ä»·æ ¼: Â¥{summary['predicted_price']:.2f}")
                
                return True
            else:
                print(f"âŒ é¢„æµ‹å¤±è´¥: {data.get('error')}")
                return False
        else:
            print(f"âŒ é¢„æµ‹è¯·æ±‚å¤±è´¥: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ APIæµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def monitor_gpu_usage():
    """ç›‘æ§GPUä½¿ç”¨æƒ…å†µ"""
    print("\nğŸ“Š GPUä½¿ç”¨ç›‘æ§")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("âŒ GPUä¸å¯ç”¨")
        return
    
    try:
        # è·å–GPUä¿¡æ¯
        device = torch.device("cuda:0")
        
        # å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory_allocated = torch.cuda.memory_allocated(device) / 1024**3
        memory_reserved = torch.cuda.memory_reserved(device) / 1024**3
        memory_total = torch.cuda.get_device_properties(device).total_memory / 1024**3
        
        print(f"ğŸ“± GPUè®¾å¤‡: {torch.cuda.get_device_name(device)}")
        print(f"ğŸ’¾ å·²åˆ†é…å†…å­˜: {memory_allocated:.2f} GB")
        print(f"ğŸ’¾ å·²ä¿ç•™å†…å­˜: {memory_reserved:.2f} GB")
        print(f"ğŸ’¾ æ€»å†…å­˜: {memory_total:.2f} GB")
        print(f"ğŸ“Š å†…å­˜ä½¿ç”¨ç‡: {(memory_allocated/memory_total)*100:.1f}%")
        
        # GPUåˆ©ç”¨ç‡ï¼ˆéœ€è¦nvidia-ml-pyåº“ï¼‰
        try:
            import pynvml
            pynvml.nvmlInit()
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
            temperature = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
            
            print(f"ğŸ”¥ GPUåˆ©ç”¨ç‡: {utilization.gpu}%")
            print(f"ğŸŒ¡ï¸ GPUæ¸©åº¦: {temperature}Â°C")
            
        except ImportError:
            print("ğŸ’¡ å®‰è£… nvidia-ml-py å¯è·å–æ›´è¯¦ç»†çš„GPUä¿¡æ¯")
            print("   å‘½ä»¤: pip install nvidia-ml-py")
        
    except Exception as e:
        print(f"âŒ GPUç›‘æ§å¤±è´¥: {str(e)}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ GPUæ€§èƒ½æµ‹è¯•å’Œé…ç½®")
    print("=" * 60)
    
    # 1. æ£€æµ‹GPU
    gpu_available = test_gpu_availability()
    
    if gpu_available:
        # 2. æ€§èƒ½å¯¹æ¯”
        benchmark_cpu_vs_gpu()
        
        # 3. GPUç›‘æ§
        monitor_gpu_usage()
        
        # 4. APIæµ‹è¯•
        test_api_with_gpu()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ GPUé…ç½®å®Œæˆï¼")
        
        print(f"\nâœ… GPUé…ç½®æ€»ç»“:")
        print(f"   ğŸš€ GPUå‹å·: {torch.cuda.get_device_name(0)}")
        print(f"   ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"   âš¡ CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"   ğŸ“¦ PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        print(f"\nğŸ”§ ä½¿ç”¨GPUçš„å¥½å¤„:")
        print(f"   âš¡ é¢„æµ‹é€Ÿåº¦æ˜¾è‘—æå‡")
        print(f"   ğŸ§  æ”¯æŒæ›´å¤§çš„æ¨¡å‹")
        print(f"   ğŸ“Š å¹¶è¡Œå¤„ç†èƒ½åŠ›å¼º")
        print(f"   ğŸš€ é€‚åˆæ‰¹é‡é¢„æµ‹")
        
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"   1. é‡å¯APIæœåŠ¡åº”ç”¨GPUé…ç½®")
        print(f"   2. æµ‹è¯•é¢„æµ‹æ€§èƒ½æå‡")
        print(f"   3. ç›‘æ§GPUä½¿ç”¨æƒ…å†µ")
        
    else:
        print("\nâŒ GPUé…ç½®å¤±è´¥")
        print("è¯·æ£€æŸ¥CUDAå®‰è£…å’ŒPyTorchç‰ˆæœ¬")

if __name__ == "__main__":
    main()
