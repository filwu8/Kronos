#!/usr/bin/env python3
"""
GPUå°±ç»ªçŠ¶æ€æ£€æŸ¥
"""

import torch
import subprocess
import sys

def check_gpu_status():
    """æ£€æŸ¥GPUçŠ¶æ€"""
    print("ğŸ” GPUå°±ç»ªçŠ¶æ€æ£€æŸ¥")
    print("=" * 50)
    
    # 1. ç¡¬ä»¶æ£€æµ‹
    print("\n1. ğŸ–¥ï¸ ç¡¬ä»¶æ£€æµ‹:")
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,driver_version', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            gpu_info = result.stdout.strip().split(', ')
            print(f"   GPUå‹å·: {gpu_info[0]}")
            print(f"   æ˜¾å­˜å¤§å°: {gpu_info[1]} MB")
            print(f"   é©±åŠ¨ç‰ˆæœ¬: {gpu_info[2]}")
        else:
            print("   âŒ æ— æ³•è·å–GPUä¿¡æ¯")
            return False
    except FileNotFoundError:
        print("   âŒ nvidia-smi æœªæ‰¾åˆ°")
        return False
    
    # 2. CUDAæ£€æµ‹
    print("\n2. ğŸ”§ CUDAæ£€æµ‹:")
    print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
        
        # 3. å…¼å®¹æ€§æµ‹è¯•
        print("\n3. ğŸ§ª å…¼å®¹æ€§æµ‹è¯•:")
        try:
            device = torch.device("cuda:0")
            x = torch.randn(100, 100, device=device)
            y = torch.randn(100, 100, device=device)
            z = torch.mm(x, y)
            print("   âœ… GPUè®¡ç®—æµ‹è¯•é€šè¿‡")
            
            # æ¸…ç†
            del x, y, z
            torch.cuda.empty_cache()
            return True
            
        except Exception as e:
            print(f"   âŒ GPUè®¡ç®—æµ‹è¯•å¤±è´¥: {str(e)}")
            
            # RTX 5090ç‰¹æ®Šå¤„ç†
            if "sm_120" in str(e) or "no kernel image" in str(e):
                print("\nğŸ’¡ RTX 5090å…¼å®¹æ€§è¯´æ˜:")
                print("   æ‚¨çš„RTX 5090æ˜¯æœ€æ–°GPUï¼Œå½“å‰PyTorchç‰ˆæœ¬ä¸å®Œå…¨æ”¯æŒ")
                print("   å»ºè®®è§£å†³æ–¹æ¡ˆ:")
                print("   1. ç­‰å¾…PyTorch 2.5+ç‰ˆæœ¬å‘å¸ƒ")
                print("   2. ä½¿ç”¨PyTorch nightlyç‰ˆæœ¬")
                print("   3. å½“å‰ä½¿ç”¨CPUä¼˜åŒ–æ¨¡å¼")
                
            return False
    else:
        print("   âŒ CUDAä¸å¯ç”¨")
        return False

def check_pytorch_versions():
    """æ£€æŸ¥PyTorchç‰ˆæœ¬é€‰é¡¹"""
    print("\n4. ğŸ“¦ PyTorchç‰ˆæœ¬é€‰é¡¹:")
    
    print("   å½“å‰ç‰ˆæœ¬: PyTorch 2.4.1+cu124")
    print("   RTX 5090æ”¯æŒçŠ¶æ€:")
    print("   - PyTorch 2.4.x: âŒ ä¸æ”¯æŒ (å½“å‰)")
    print("   - PyTorch 2.5.x: âœ… é¢„è®¡æ”¯æŒ")
    print("   - PyTorch Nightly: ğŸ”„ éƒ¨åˆ†æ”¯æŒ")
    
    print("\nğŸ’¡ å‡çº§å»ºè®®:")
    print("   ç­‰å¾…PyTorch 2.5æ­£å¼ç‰ˆå‘å¸ƒ")
    print("   æˆ–å°è¯•: pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu124")

def cpu_optimization_info():
    """CPUä¼˜åŒ–ä¿¡æ¯"""
    print("\n5. ğŸš€ CPUä¼˜åŒ–é…ç½®:")
    
    # CPUä¿¡æ¯
    import os
    cpu_count = os.cpu_count()
    print(f"   CPUæ ¸å¿ƒæ•°: {cpu_count}")
    print(f"   æ¨èçº¿ç¨‹æ•°: {min(8, cpu_count)}")
    
    # å½“å‰PyTorchè®¾ç½®
    print(f"   å½“å‰çº¿ç¨‹æ•°: {torch.get_num_threads()}")
    
    print("\nâœ… CPUä¼˜åŒ–å·²å¯ç”¨:")
    print("   - å¤šçº¿ç¨‹å¹¶è¡Œè®¡ç®—")
    print("   - å†…å­˜ä¼˜åŒ–")
    print("   - æ‰¹å¤„ç†ä¼˜åŒ–")

def future_gpu_setup():
    """æœªæ¥GPUè®¾ç½®æŒ‡å—"""
    print("\n6. ğŸ”® GPUå°±ç»ªè®¡åˆ’:")
    
    print("   å½“PyTorchæ”¯æŒRTX 5090åï¼Œç³»ç»Ÿå°†è‡ªåŠ¨:")
    print("   âœ… æ£€æµ‹GPUå…¼å®¹æ€§")
    print("   âœ… è‡ªåŠ¨åˆ‡æ¢åˆ°GPUæ¨¡å¼")
    print("   âœ… æ˜¾è‘—æå‡é¢„æµ‹é€Ÿåº¦")
    print("   âœ… æ”¯æŒæ›´å¤§æ‰¹é‡é¢„æµ‹")
    
    print("\nğŸ“Š é¢„æœŸæ€§èƒ½æå‡:")
    print("   - é¢„æµ‹é€Ÿåº¦: 5-10å€æå‡")
    print("   - æ‰¹é‡å¤„ç†: æ”¯æŒæ›´å¤šè‚¡ç¥¨")
    print("   - æ¨¡å‹åŠ è½½: æ›´å¿«å¯åŠ¨")
    print("   - å†…å­˜ä½¿ç”¨: 31.8GB GPUå†…å­˜")

def main():
    """ä¸»å‡½æ•°"""
    gpu_ready = check_gpu_status()
    check_pytorch_versions()
    cpu_optimization_info()
    future_gpu_setup()
    
    print("\n" + "=" * 50)
    print("ğŸ“Š GPUå°±ç»ªçŠ¶æ€æ€»ç»“:")
    
    if gpu_ready:
        print("ğŸ‰ GPUå®Œå…¨å°±ç»ªï¼Œç³»ç»Ÿå°†ä½¿ç”¨GPUåŠ é€Ÿ")
        print("\nğŸš€ GPUé…ç½®:")
        print(f"   è®¾å¤‡: {torch.cuda.get_device_name(0)}")
        print(f"   å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print("   çŠ¶æ€: âœ… å¯ç”¨")
        
    else:
        print("âš ï¸ GPUæš‚ä¸å¯ç”¨ï¼Œä½¿ç”¨CPUä¼˜åŒ–æ¨¡å¼")
        print("\nğŸ–¥ï¸ CPUé…ç½®:")
        print("   å¤šçº¿ç¨‹: âœ… å·²å¯ç”¨")
        print("   ä¼˜åŒ–: âœ… å·²é…ç½®")
        print("   æ€§èƒ½: ğŸš€ å·²ä¼˜åŒ–")
        
        print("\nğŸ’¡ GPUå¯ç”¨è®¡åˆ’:")
        print("   1. ç­‰å¾…PyTorch 2.5å‘å¸ƒ")
        print("   2. å‡çº§PyTorchç‰ˆæœ¬")
        print("   3. ç³»ç»Ÿè‡ªåŠ¨åˆ‡æ¢åˆ°GPU")
        print("   4. äº«å—5-10å€æ€§èƒ½æå‡")
    
    print(f"\nğŸ”§ å½“å‰ç³»ç»ŸçŠ¶æ€:")
    print(f"   âœ… è‚¡ç¥¨é¢„æµ‹: æ­£å¸¸å·¥ä½œ")
    print(f"   âœ… çœŸå®æ•°æ®: å·²å¯ç”¨")
    print(f"   âœ… å›¾è¡¨æ˜¾ç¤º: å·²ä¿®å¤")
    print(f"   âœ… æ€§èƒ½ä¼˜åŒ–: å·²é…ç½®")
    
    print(f"\nğŸŒ ç«‹å³ä½¿ç”¨:")
    print(f"   å‰ç«¯: http://localhost:8501")
    print(f"   API: http://localhost:8000/docs")

if __name__ == "__main__":
    main()
