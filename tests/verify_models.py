#!/usr/bin/env python3
"""
éªŒè¯ä¸‹è½½çš„Kronosæ¨¡å‹
"""

import os
import json
from pathlib import Path

def check_model_files(model_dir, model_name):
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§"""
    print(f"\nğŸ” æ£€æŸ¥ {model_name}")
    print("-" * 40)
    
    if not model_dir.exists():
        print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
        return False
    
    print(f"âœ… æ¨¡å‹ç›®å½•å­˜åœ¨: {model_dir}")
    
    # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
    required_files = {
        "config.json": "é…ç½®æ–‡ä»¶",
        "model.safetensors": "æ¨¡å‹æƒé‡æ–‡ä»¶",
        "README.md": "è¯´æ˜æ–‡æ¡£"
    }
    
    missing_files = []
    file_info = {}
    
    for file_name, description in required_files.items():
        file_path = model_dir / file_name
        if file_path.exists():
            file_size = file_path.stat().st_size
            file_info[file_name] = file_size
            print(f"âœ… {description}: {file_name} ({file_size:,} bytes)")
        else:
            print(f"âŒ {description}: {file_name} (ç¼ºå¤±)")
            missing_files.append(file_name)
    
    # è¯»å–é…ç½®æ–‡ä»¶
    config_path = model_dir / "config.json"
    if config_path.exists():
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            print(f"\nğŸ“‹ æ¨¡å‹é…ç½®:")
            for key, value in config.items():
                print(f"   {key}: {value}")
                
        except Exception as e:
            print(f"âŒ é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
    
    # æ£€æŸ¥README
    readme_path = model_dir / "README.md"
    if readme_path.exists():
        try:
            with open(readme_path, 'r', encoding='utf-8') as f:
                readme_content = f.read()
            
            print(f"\nğŸ“– READMEä¿¡æ¯:")
            # æ˜¾ç¤ºå‰å‡ è¡Œ
            lines = readme_content.split('\n')[:10]
            for line in lines:
                if line.strip():
                    print(f"   {line}")
                    
        except Exception as e:
            print(f"âŒ READMEè¯»å–å¤±è´¥: {str(e)}")
    
    if missing_files:
        print(f"\nâš ï¸ ç¼ºå°‘æ–‡ä»¶: {missing_files}")
        return False
    else:
        print(f"\nğŸ‰ {model_name} æ–‡ä»¶å®Œæ•´")
        return True

def analyze_model_sizes():
    """åˆ†ææ¨¡å‹å¤§å°"""
    print(f"\nğŸ“Š æ¨¡å‹å¤§å°åˆ†æ")
    print("=" * 40)
    
    models_dir = Path('volumes/models')
    total_size = 0
    
    for model_subdir in models_dir.iterdir():
        if model_subdir.is_dir():
            model_size = sum(f.stat().st_size for f in model_subdir.rglob('*') if f.is_file())
            total_size += model_size
            
            print(f"ğŸ“¦ {model_subdir.name}: {model_size:,} bytes ({model_size/1024/1024:.1f} MB)")
    
    print(f"\nğŸ“¦ æ€»å¤§å°: {total_size:,} bytes ({total_size/1024/1024:.1f} MB)")
    
    return total_size

def check_model_compatibility():
    """æ£€æŸ¥æ¨¡å‹å…¼å®¹æ€§"""
    print(f"\nğŸ”§ æ¨¡å‹å…¼å®¹æ€§æ£€æŸ¥")
    print("=" * 40)
    
    # æ£€æŸ¥Tokenizeré…ç½®
    tokenizer_config_path = Path("models/Kronos-Tokenizer-base/config.json")
    predictor_config_path = Path("models/Kronos-small/config.json")
    
    compatibility_issues = []
    
    if tokenizer_config_path.exists() and predictor_config_path.exists():
        try:
            with open(tokenizer_config_path, 'r') as f:
                tokenizer_config = json.load(f)
            
            with open(predictor_config_path, 'r') as f:
                predictor_config = json.load(f)
            
            # æ£€æŸ¥å…³é”®å‚æ•°åŒ¹é…
            print("ğŸ” æ£€æŸ¥å…³é”®å‚æ•°åŒ¹é…:")
            
            # æ£€æŸ¥bitså‚æ•°
            if 's1_bits' in tokenizer_config and 's1_bits' in predictor_config:
                if tokenizer_config['s1_bits'] == predictor_config['s1_bits']:
                    print(f"âœ… s1_bits åŒ¹é…: {tokenizer_config['s1_bits']}")
                else:
                    issue = f"s1_bits ä¸åŒ¹é…: Tokenizer={tokenizer_config['s1_bits']}, Predictor={predictor_config['s1_bits']}"
                    print(f"âŒ {issue}")
                    compatibility_issues.append(issue)
            
            if 's2_bits' in tokenizer_config and 's2_bits' in predictor_config:
                if tokenizer_config['s2_bits'] == predictor_config['s2_bits']:
                    print(f"âœ… s2_bits åŒ¹é…: {tokenizer_config['s2_bits']}")
                else:
                    issue = f"s2_bits ä¸åŒ¹é…: Tokenizer={tokenizer_config['s2_bits']}, Predictor={predictor_config['s2_bits']}"
                    print(f"âŒ {issue}")
                    compatibility_issues.append(issue)
            
            # æ£€æŸ¥è¾“å…¥ç»´åº¦
            if 'd_in' in tokenizer_config:
                d_in = tokenizer_config['d_in']
                print(f"âœ… è¾“å…¥ç»´åº¦: {d_in} (åº”è¯¥åŒ¹é…æ•°æ®ç‰¹å¾æ•°)")
                
                # æ£€æŸ¥æ˜¯å¦ä¸æˆ‘ä»¬çš„æ•°æ®å…¼å®¹
                expected_features = 6  # open, high, low, close, volume, amount
                if d_in == expected_features:
                    print(f"âœ… è¾“å…¥ç»´åº¦ä¸æ•°æ®ç‰¹å¾åŒ¹é…: {d_in}")
                else:
                    issue = f"è¾“å…¥ç»´åº¦ä¸åŒ¹é…: æ¨¡å‹æœŸæœ›{d_in}, æ•°æ®æœ‰{expected_features}ä¸ªç‰¹å¾"
                    print(f"âš ï¸ {issue}")
                    compatibility_issues.append(issue)
            
        except Exception as e:
            print(f"âŒ é…ç½®æ–‡ä»¶è§£æå¤±è´¥: {str(e)}")
            compatibility_issues.append(f"é…ç½®æ–‡ä»¶è§£æå¤±è´¥: {str(e)}")
    
    if compatibility_issues:
        print(f"\nâš ï¸ å‘ç°å…¼å®¹æ€§é—®é¢˜:")
        for issue in compatibility_issues:
            print(f"   - {issue}")
        return False
    else:
        print(f"\nâœ… æ¨¡å‹å…¼å®¹æ€§æ£€æŸ¥é€šè¿‡")
        return True

def update_config_files():
    """æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹è·¯å¾„"""
    print(f"\nğŸ“ æ›´æ–°é…ç½®æ–‡ä»¶")
    print("=" * 40)
    
    # æ›´æ–°finetune/config.py
    config_file = Path("finetune/config.py")
    
    if config_file.exists():
        try:
            # è¯»å–åŸæ–‡ä»¶
            with open(config_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # å¤‡ä»½åŸæ–‡ä»¶
            backup_file = config_file.with_suffix('.py.backup')
            with open(backup_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            # æ›´æ–°æ¨¡å‹è·¯å¾„
            updated_content = content.replace(
                'self.pretrained_tokenizer_path = "path/to/your/Kronos-Tokenizer-base"',
                'self.pretrained_tokenizer_path = "./models/Kronos-Tokenizer-base"'
            ).replace(
                'self.pretrained_predictor_path = "path/to/your/Kronos-small"',
                'self.pretrained_predictor_path = "./models/Kronos-small"'
            )
            
            # å†™å…¥æ›´æ–°åçš„å†…å®¹
            with open(config_file, 'w', encoding='utf-8') as f:
                f.write(updated_content)
            
            print(f"âœ… å·²æ›´æ–° {config_file}")
            print(f"âœ… åŸæ–‡ä»¶å¤‡ä»½åˆ° {backup_file}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ›´æ–°é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
            return False
    else:
        print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– éªŒè¯Kronosæ¨¡å‹ä¸‹è½½ç»“æœ")
    print("=" * 60)
    
    models_dir = Path('volumes/models')
    
    if not models_dir.exists():
        print("âŒ modelsç›®å½•ä¸å­˜åœ¨")
        return 1
    
    # æ£€æŸ¥å„ä¸ªæ¨¡å‹
    tokenizer_ok = check_model_files(
        models_dir / "Kronos-Tokenizer-base", 
        "Kronos-Tokenizer-base"
    )
    
    predictor_ok = check_model_files(
        models_dir / "Kronos-small", 
        "Kronos-small"
    )
    
    # åˆ†ææ¨¡å‹å¤§å°
    total_size = analyze_model_sizes()
    
    # æ£€æŸ¥å…¼å®¹æ€§
    compatibility_ok = check_model_compatibility()
    
    # æ›´æ–°é…ç½®æ–‡ä»¶
    config_updated = update_config_files()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š éªŒè¯ç»“æœæ€»ç»“")
    
    results = [
        ("Kronos-Tokenizer-base", tokenizer_ok),
        ("Kronos-small", predictor_ok),
        ("æ¨¡å‹å…¼å®¹æ€§", compatibility_ok),
        ("é…ç½®æ–‡ä»¶æ›´æ–°", config_updated)
    ]
    
    all_ok = True
    for item, status in results:
        icon = "âœ…" if status else "âŒ"
        print(f"   {icon} {item}: {'å°±ç»ª' if status else 'æœ‰é—®é¢˜'}")
        if not status:
            all_ok = False
    
    if all_ok:
        print(f"\nğŸ‰ æ‰€æœ‰æ¨¡å‹éªŒè¯é€šè¿‡ï¼")
        print(f"\nğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
        print(f"   ğŸ“¦ æ€»å¤§å°: {total_size/1024/1024:.1f} MB")
        print(f"   ğŸ”¤ Tokenizer: models/Kronos-Tokenizer-base/")
        print(f"   ğŸ§  Predictor: models/Kronos-small/")
        
        print(f"\nğŸ”§ åç»­æ­¥éª¤:")
        print(f"   1. åˆ›å»ºæ•°æ®é€‚é…å™¨è¿æ¥akshareæ•°æ®")
        print(f"   2. æ›´æ–°é¢„æµ‹æœåŠ¡ä½¿ç”¨çœŸå®æ¨¡å‹")
        print(f"   3. æµ‹è¯•ç«¯åˆ°ç«¯é¢„æµ‹æµç¨‹")
        print(f"   4. é‡å¯åº”ç”¨éªŒè¯æ•ˆæœ")
        
    else:
        print(f"\nâš ï¸ éƒ¨åˆ†éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³é—®é¢˜")
        print(f"\nğŸ”§ æ•…éšœæ’é™¤:")
        print(f"   1. é‡æ–°ä¸‹è½½ç¼ºå¤±çš„æ¨¡å‹æ–‡ä»¶")
        print(f"   2. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print(f"   3. éªŒè¯Git LFSæ˜¯å¦æ­£å¸¸å·¥ä½œ")
    
    return 0 if all_ok else 1

if __name__ == "__main__":
    exit(main())
