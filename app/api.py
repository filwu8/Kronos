"""
FastAPIåç«¯æœåŠ¡
æä¾›è‚¡ç¥¨é¢„æµ‹APIæ¥å£
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import logging
from datetime import datetime
import os

from .prediction_service import get_prediction_service

# é…ç½®æ—¥å¿—ï¼ˆæ”¯æŒç¯å¢ƒå˜é‡ LOG_LEVELï¼›å¯é€‰è½ç›˜åˆ° volumes/logs/api_server.logï¼‰
log_level = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(level=getattr(logging, log_level, logging.INFO))
logger = logging.getLogger(__name__)

try:
    from pathlib import Path
    log_dir = Path("volumes") / "logs"
    log_dir.mkdir(parents=True, exist_ok=True)
    fh = logging.FileHandler(log_dir / "api_server.log", encoding="utf-8")
    fh.setLevel(getattr(logging, log_level, logging.INFO))
    fmt = logging.Formatter("%(asctime)s [%(levelname)s] %(name)s: %(message)s")
    fh.setFormatter(fmt)
    logger.addHandler(fh)
except Exception:
    pass

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="Gordon Wang çš„è‚¡ç¥¨é¢„æµ‹API",
    description="åŸºäºRTX 5090 GPUåŠ é€Ÿçš„æ™ºèƒ½è‚¡ç¥¨ä»·æ ¼é¢„æµ‹æœåŠ¡",
    version="1.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡
prediction_service = None


# Pydanticæ¨¡å‹å®šä¹‰
class PredictionRequest(BaseModel):
    """é¢„æµ‹è¯·æ±‚æ¨¡å‹"""
    stock_code: str = Field(..., description="è‚¡ç¥¨ä»£ç ï¼Œå¦‚ï¼š000001ã€600000")
    period: str = Field("1y", description="å†å²æ•°æ®å‘¨æœŸï¼š1y, 2y, 5y")
    pred_len: int = Field(30, ge=1, le=120, description="é¢„æµ‹å¤©æ•°ï¼Œ1-120å¤©")
    lookback: int = Field(1000, ge=50, le=5000, description="å†å²æ•°æ®é•¿åº¦ï¼ŒRTX 5090æ”¯æŒå¤§æ•°æ®é‡")
    temperature: float = Field(1.0, ge=0.1, le=2.0, description="é‡‡æ ·æ¸©åº¦")
    top_p: float = Field(0.9, ge=0.1, le=1.0, description="æ ¸é‡‡æ ·æ¦‚ç‡")
    sample_count: int = Field(1, ge=1, le=10, description="é‡‡æ ·æ¬¡æ•°ï¼Œé«˜æ€§èƒ½æ¨¡å¼æ”¯æŒæ›´å¤š")
    debug: bool = Field(False, description="è°ƒè¯•æ¨¡å¼ï¼šè¿”å›åŸå§‹é¢„æµ‹(æœªçº¦æŸ)ç”¨äºè¯Šæ–­")


class BatchPredictionRequest(BaseModel):
    """æ‰¹é‡é¢„æµ‹è¯·æ±‚æ¨¡å‹"""
    stock_codes: List[str] = Field(..., description="è‚¡ç¥¨ä»£ç åˆ—è¡¨")
    period: str = Field("1y", description="å†å²æ•°æ®å‘¨æœŸ")
    pred_len: int = Field(30, ge=1, le=60, description="é¢„æµ‹å¤©æ•°")


class StockInfo(BaseModel):
    """è‚¡ç¥¨ä¿¡æ¯æ¨¡å‹"""
    code: str
    name: str
    market: str
    source: str


class PredictionSummary(BaseModel):
    """é¢„æµ‹æ‘˜è¦æ¨¡å‹"""
    current_price: float
    predicted_price: float
    change_amount: float
    change_percent: float
    trend: str
    volatility: float
    prediction_days: int
    confidence: str


class PredictionResponse(BaseModel):
    """é¢„æµ‹å“åº”æ¨¡å‹"""
    success: bool
    error: Optional[str] = None
    data: Optional[Dict[str, Any]] = None


# å¯åŠ¨äº‹ä»¶
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–"""
    global prediction_service

    logger.info("æ­£åœ¨å¯åŠ¨è‚¡ç¥¨é¢„æµ‹æœåŠ¡...")

    # å¼ºåˆ¶ä½¿ç”¨çœŸå®æ•°æ®æ¨¡å¼
    use_mock = False  # å¼ºåˆ¶å…³é—­æ¨¡æ‹Ÿæ¨¡å¼

    # è®¾å¤‡é€‰æ‹©ï¼šä¼˜å…ˆè¯»å– DEVICE ç¯å¢ƒå˜é‡ï¼ˆauto/cpu/cudaï¼‰ï¼Œauto æ—¶æŒ‰å¯ç”¨æ€§é€‰æ‹©
    import torch
    prefer = os.getenv("DEVICE", "auto").lower()
    device = "cpu"
    if prefer == "cuda":
        if torch.cuda.is_available():
            device = "cuda"
        else:
            logger.warning("æŒ‡å®š DEVICE=cuda ä½† CUDA ä¸å¯ç”¨ï¼Œå›é€€åˆ° CPU")
    elif prefer == "cpu":
        device = "cpu"
    else:
        # auto æ¨¡å¼
        device = "cuda" if torch.cuda.is_available() else "cpu"

    # å¦‚é€‰æ‹©ä¸º CUDAï¼Œåšä¸€æ¬¡æå°è®¡ç®—çƒŸé›¾æµ‹è¯•ï¼Œé¿å…ä¸å…¼å®¹æ¶æ„å¯¼è‡´è¿è¡Œæ—¶é”™è¯¯
    if device == "cuda":
        try:
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"æ£€æµ‹åˆ°GPU: {gpu_name}")
            logger.info(f"GPUå†…å­˜: {gpu_memory:.1f} GB")
            torch.zeros((1, 1), device="cuda").matmul(torch.ones((1, 1), device="cuda"))
            logger.info("GPUçƒŸé›¾æµ‹è¯•é€šè¿‡ï¼Œä½¿ç”¨GPUè¿è¡Œ")
        except Exception as e:
            logger.warning(f"GPUçƒŸé›¾æµ‹è¯•å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
            device = "cpu"
    else:
        logger.info("æœªé€‰æ‹©æˆ–æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPU")

    # CPUä¼˜åŒ–é…ç½®
    if device == "cpu":
        try:
            cpu_threads = int(os.getenv('CPU_THREADS', max(1, (os.cpu_count() or 4) // 2)))
            torch.set_num_threads(cpu_threads)
            logger.info(f"ğŸš€ CPUä¼˜åŒ–ï¼šä½¿ç”¨{cpu_threads}çº¿ç¨‹å¹¶è¡Œè®¡ç®—")
        except Exception as e:
            logger.warning(f"è®¾ç½®CPUçº¿ç¨‹å¤±è´¥: {e}")

    try:
        prediction_service = get_prediction_service(device=device, use_mock=use_mock)
        logger.info("é¢„æµ‹æœåŠ¡å¯åŠ¨æˆåŠŸ - ä½¿ç”¨çœŸå®æ•°æ®æ¨¡å¼")
    except Exception as e:
        logger.error(f"é¢„æµ‹æœåŠ¡å¯åŠ¨å¤±è´¥: {str(e)}")
        # å³ä½¿å¤±è´¥ä¹Ÿè¦å¯åŠ¨ï¼Œä½†ä»å°è¯•çœŸå®æ¨¡å¼
        prediction_service = get_prediction_service(device="cpu", use_mock=False)


# APIè·¯ç”±
@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "Kronosè‚¡ç¥¨é¢„æµ‹API",
        "version": "1.0.0",
        "status": "running",
        "timestamp": datetime.now().isoformat()
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    global prediction_service

    if prediction_service is None:
        raise HTTPException(status_code=503, detail="é¢„æµ‹æœåŠ¡æœªåˆå§‹åŒ–")

    status = prediction_service.get_model_status()

    return {
        "status": "healthy",
        "model_status": status,
        "timestamp": datetime.now().isoformat()
    }


@app.get("/model/status")
async def model_status():
    """è¿”å›æ¨¡å‹ä¸æ•°æ®è·å–çš„å½“å‰çŠ¶æ€ï¼ˆä¾›å‰ç«¯è½®è¯¢å±•ç¤ºï¼‰"""
    global prediction_service
    if prediction_service is None:
        raise HTTPException(status_code=503, detail="é¢„æµ‹æœåŠ¡æœªåˆå§‹åŒ–")
    return prediction_service.get_model_status()


@app.post("/refresh/{stock_code}")
async def refresh_stock(stock_code: str, period: str = "1y"):
    """å¢é‡åˆ·æ–°æŸåªè‚¡ç¥¨çš„ç¼“å­˜ï¼Œæ”¯æŒæŒ‡å®š periodï¼ˆé»˜è®¤1yï¼‰"""
    global prediction_service
    if prediction_service is None:
        raise HTTPException(status_code=503, detail="é¢„æµ‹æœåŠ¡æœªåˆå§‹åŒ–")
    try:
        fetcher = prediction_service.data_fetcher
        info = fetcher.refresh_stock_cache(stock_code, period=period)
        if not info:
            raise HTTPException(status_code=502, detail="åœ¨çº¿æ•°æ®æºæ— è¿”å›æˆ–è§£æå¤±è´¥")
        return {"success": True, "data": info}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/predict", response_model=PredictionResponse)
async def predict_stock(request: PredictionRequest):
    """
    é¢„æµ‹å•åªè‚¡ç¥¨ä»·æ ¼
    """
    global prediction_service
    import time

    if prediction_service is None:
        raise HTTPException(status_code=503, detail="é¢„æµ‹æœåŠ¡æœªåˆå§‹åŒ–")

    start_time = time.time()
    try:
        logger.info(f"æ”¶åˆ°é¢„æµ‹è¯·æ±‚: {request.stock_code}, å‚æ•°: lookback={request.lookback}, sample_count={request.sample_count}, pred_len={request.pred_len}")

        # æ‰§è¡Œé¢„æµ‹
        result = prediction_service.predict_stock(
            stock_code=request.stock_code,
            period=request.period,
            pred_len=request.pred_len,
            lookback=request.lookback,
            T=request.temperature,
            top_p=request.top_p,
            sample_count=request.sample_count,
            debug=request.debug
        )

        elapsed_time = time.time() - start_time
        logger.info(f"é¢„æµ‹å®Œæˆ: {request.stock_code}, è€—æ—¶: {elapsed_time:.2f}ç§’")

        if not result['success']:
            raise HTTPException(status_code=400, detail=result['error'])

        # æ·»åŠ æ€§èƒ½ä¿¡æ¯åˆ°å“åº”
        if result.get('data'):
            result['data']['performance'] = {
                'elapsed_time': round(elapsed_time, 2),
                'parameters': {
                    'lookback': request.lookback,
                    'sample_count': request.sample_count,
                    'pred_len': request.pred_len
                }
            }

        return PredictionResponse(**result)

    except HTTPException:
        raise
    except Exception as e:
        elapsed_time = time.time() - start_time
        logger.error(f"é¢„æµ‹è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}, è€—æ—¶: {elapsed_time:.2f}ç§’")
        raise HTTPException(status_code=500, detail=f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {str(e)}")


@app.post("/predict/batch")
async def batch_predict(request: BatchPredictionRequest):
    """
    æ‰¹é‡é¢„æµ‹å¤šåªè‚¡ç¥¨
    """
    global prediction_service

    if prediction_service is None:
        raise HTTPException(status_code=503, detail="é¢„æµ‹æœåŠ¡æœªåˆå§‹åŒ–")

    if len(request.stock_codes) > 10:
        raise HTTPException(status_code=400, detail="æ‰¹é‡é¢„æµ‹æœ€å¤šæ”¯æŒ10åªè‚¡ç¥¨")

    try:
        logger.info(f"æ”¶åˆ°æ‰¹é‡é¢„æµ‹è¯·æ±‚: {request.stock_codes}")

        # æ‰§è¡Œæ‰¹é‡é¢„æµ‹
        results = prediction_service.batch_predict(
            stock_codes=request.stock_codes,
            period=request.period,
            pred_len=request.pred_len
        )

        return {
            "success": True,
            "data": results,
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"æ‰¹é‡é¢„æµ‹è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {str(e)}")


@app.get("/stocks/{stock_code}/info")
async def get_stock_info(stock_code: str):
    """
    è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
    """
    global prediction_service

    if prediction_service is None:
        raise HTTPException(status_code=503, detail="é¢„æµ‹æœåŠ¡æœªåˆå§‹åŒ–")

    try:
        info = prediction_service.data_fetcher.get_stock_info(stock_code)
        return {
            "success": True,
            "data": info,
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨ä¿¡æ¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–è‚¡ç¥¨ä¿¡æ¯å¤±è´¥: {str(e)}")


@app.get("/stocks/{stock_code}/history")
async def get_stock_history(
    stock_code: str,
    period: str = "1y",
    limit: int = 100
):
    """
    è·å–è‚¡ç¥¨å†å²æ•°æ®
    """
    global prediction_service

    if prediction_service is None:
        raise HTTPException(status_code=503, detail="é¢„æµ‹æœåŠ¡æœªåˆå§‹åŒ–")

    try:
        df = prediction_service.data_fetcher.fetch_stock_data(stock_code, period=period)

        if df is None or df.empty:
            raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°è‚¡ç¥¨æ•°æ®: {stock_code}")

        # é™åˆ¶è¿”å›æ•°æ®é‡
        if len(df) > limit:
            df = df.tail(limit)

        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        data = df.reset_index().to_dict('records')

        return {
            "success": True,
            "data": {
                "stock_code": stock_code,
                "period": period,
                "count": len(data),
                "history": data
            },
            "timestamp": datetime.now().isoformat()
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–å†å²æ•°æ®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–å†å²æ•°æ®å¤±è´¥: {str(e)}")


@app.get("/model/status")
async def get_model_status():
    """
    è·å–æ¨¡å‹çŠ¶æ€ä¿¡æ¯
    """
    global prediction_service

    if prediction_service is None:
        return {
            "model_loaded": False,
            "error": "é¢„æµ‹æœåŠ¡æœªåˆå§‹åŒ–"
        }

    status = prediction_service.get_model_status()
    return status


# å®æ—¶ç³»ç»Ÿèµ„æºç›‘æ§ï¼ˆCPU/GPUï¼‰
@app.get("/metrics/usage")
async def get_system_usage():
    """è¿”å›å½“å‰ CPU æˆ– GPU çš„å®æ—¶åˆ©ç”¨ç‡ä¸å†…å­˜å ç”¨ï¼ˆè½»é‡é‡‡æ ·ï¼‰"""
    try:
        import psutil
        import time
        import torch
    except Exception as e:
        return {"success": False, "error": f"ä¾èµ–ç¼ºå¤±: {e}"}

    # CPU åŸºç¡€ä¿¡æ¯
    cpu_percent = psutil.cpu_percent(interval=0.3)  # è½»é‡é˜»å¡ 0.3s é‡‡æ ·
    mem = psutil.virtual_memory()
    mem_percent = mem.percent
    mem_used_gb = round(mem.used / 1024**3, 2)
    mem_total_gb = round(mem.total / 1024**3, 2)

    usage = {
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "timestamp": datetime.now().isoformat(),
        "cpu": {
            "percent": cpu_percent,
            "mem_percent": mem_percent,
            "mem_used_gb": mem_used_gb,
            "mem_total_gb": mem_total_gb,
        }
    }

    # è‹¥å¯ç”¨ï¼Œè¡¥å…… GPU ä¿¡æ¯
    if torch.cuda.is_available():
        gpu_info = {"available": True}
        try:
            # æ˜¾å­˜ä½¿ç”¨
            device = torch.device("cuda:0")
            allocated_gb = round(torch.cuda.memory_allocated(device) / 1024**3, 2)
            reserved_gb = round(torch.cuda.memory_reserved(device) / 1024**3, 2)
            total_gb = round(torch.cuda.get_device_properties(device).total_memory / 1024**3, 2)

            # åˆ©ç”¨ç‡ï¼ˆä¼˜å…ˆ NVMLï¼Œå¦‚æ— åˆ™ä»…è¿”å›æ˜¾å­˜ï¼‰
            util_percent = None
            temperature = None
            try:
                import pynvml
                pynvml.nvmlInit()
                handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                util_percent = int(util.gpu)
                temperature = int(pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU))
                pynvml.nvmlShutdown()
            except Exception:
                # æ²¡æœ‰ NVML æ—¶ï¼Œä»è¿”å›æ˜¾å­˜å ç”¨ä¿¡æ¯
                pass

            gpu_info.update({
                "name": torch.cuda.get_device_name(0),
                "util_percent": util_percent,  # å¯èƒ½ä¸º None è¡¨ç¤ºä¸å¯ç”¨
                "temperature": temperature,    # å¯èƒ½ä¸º None è¡¨ç¤ºä¸å¯ç”¨
                "mem_allocated_gb": allocated_gb,
                "mem_reserved_gb": reserved_gb,
                "mem_total_gb": total_gb,
                "mem_percent": round((allocated_gb / total_gb) * 100, 1) if total_gb else None,
            })
        except Exception as e:
            gpu_info.update({"error": str(e)})
        usage["gpu"] = gpu_info

    return {"success": True, "data": usage}


@app.get("/api/docs")
async def get_api_docs():
    """
    APIæ–‡æ¡£ä¿¡æ¯
    """
    return {
        "title": "Kronosè‚¡ç¥¨é¢„æµ‹API",
        "description": "åŸºäºKronosæ¨¡å‹çš„Aè‚¡è‚¡ç¥¨ä»·æ ¼é¢„æµ‹æœåŠ¡",
        "version": "1.0.0",
        "endpoints": {
            "POST /predict": "é¢„æµ‹å•åªè‚¡ç¥¨ä»·æ ¼",
            "POST /predict/batch": "æ‰¹é‡é¢„æµ‹å¤šåªè‚¡ç¥¨",
            "GET /stocks/{code}/info": "è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯",
            "GET /stocks/{code}/history": "è·å–è‚¡ç¥¨å†å²æ•°æ®",
            "GET /health": "å¥åº·æ£€æŸ¥",
            "GET /model/status": "æ¨¡å‹çŠ¶æ€"
        },
        "examples": {
            "stock_codes": ["000001", "600000", "000002.SZ", "600036.SS"],
            "prediction_request": {
                "stock_code": "000001",
                "period": "1y",
                "pred_len": 30,
                "temperature": 1.0,
                "top_p": 0.9
            }
        }
    }


# é”™è¯¯å¤„ç†
@app.exception_handler(404)
async def not_found_handler(request, exc):
    return {
        "success": False,
        "error": "æ¥å£ä¸å­˜åœ¨",
        "timestamp": datetime.now().isoformat()
    }


@app.exception_handler(500)
async def internal_error_handler(request, exc):
    return {
        "success": False,
        "error": "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯",
        "timestamp": datetime.now().isoformat()
    }


if __name__ == "__main__":
    import uvicorn

    # å¯åŠ¨æœåŠ¡
    uvicorn.run(
        "api:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
